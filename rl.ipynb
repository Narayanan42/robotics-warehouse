{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import rware\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SingleAgentRobotWarehouse(gym.Env):\n",
    "    def __init__(self, env_name=\"rware-tiny-1ag-v2\", **kwargs):\n",
    "        self.env = gym.make(env_name, **kwargs)\n",
    "        self.action_space = self.env.action_space[0]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(np.prod(self.env.observation_space[0].shape),), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        obs, info = self.env.reset(seed=seed)\n",
    "        return obs[0].flatten(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Create dummy actions for other agents (required by multi-agent API)\n",
    "        obs, rewards, terminated, truncated, info = self.env.step([action])\n",
    "        done = terminated or truncated\n",
    "        return obs[0].flatten(), rewards[0], done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(256, action_dim)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.actor(x), self.critic(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, lr=3e-4, gamma=0.99, epsilon=0.2, \n",
    "                 ent_coef=0.01, batch_size=64, n_epochs=10):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.ent_coef = ent_coef\n",
    "        self.k_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        obs_dim = self.env.observation_space.shape[0]\n",
    "        self.model = ActorCritic(obs_dim, 256, self.env.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits, value = self.model(state)\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "        return action.item(), probs.log_prob(action), value.squeeze()\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        \n",
    "        next_value = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * next_value * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * 0.95 * (1 - dones[step]) * gae\n",
    "            next_value = values[step]\n",
    "            returns.insert(0, gae + values[step])\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = torch.tensor(advantages)\n",
    "        return torch.tensor(returns), (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    def update(self, states, actions, log_probs, returns, advantages):\n",
    "        for _ in range(self.k_epochs):\n",
    "            for batch in self.get_batches(states, actions, log_probs, returns, advantages):\n",
    "                state_batch, action_batch, old_log_probs_batch, return_batch, advantage_batch = batch\n",
    "\n",
    "                # Calculate the new log probabilities and values\n",
    "                new_log_probs, values, entropy = self.model(state_batch, action_batch)\n",
    "\n",
    "                # Calculate the ratio\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs_batch)\n",
    "\n",
    "                # Calculate the surrogate losses\n",
    "                surr1 = ratio * advantage_batch\n",
    "                surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage_batch\n",
    "\n",
    "                # Calculate the actor and critic losses\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = (return_batch - values).pow(2).mean()\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                # Convert to float\n",
    "                actor_loss = actor_loss.float()\n",
    "                critic_loss = critic_loss.float()\n",
    "                entropy_loss = entropy_loss.float()\n",
    "\n",
    "                # Calculate the total loss\n",
    "                loss = actor_loss + 0.5 * critic_loss - self.ent_coef * entropy_loss\n",
    "\n",
    "                # Perform backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, total_timesteps):\n",
    "        states, actions, log_probs, rewards, dones, values = [], [], [], [], [], []\n",
    "        episode_rewards = []\n",
    "        state, _ = self.env.reset()\n",
    "\n",
    "        for _ in range(total_timesteps):\n",
    "            action, log_prob, value = self.get_action(state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob.item())\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            values.append(value.item())\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                episode_rewards.append(sum(rewards))\n",
    "                states, actions, log_probs, rewards, dones, values = [], [], [], [], [], []\n",
    "\n",
    "            if len(states) >= self.batch_size:\n",
    "                returns, advantages = self.compute_gae(rewards, values, dones)\n",
    "                self.update(states, actions, log_probs, returns, advantages)\n",
    "                states, actions, log_probs, rewards, dones, values = [], [], [], [], [], []\n",
    "\n",
    "        return episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/narayanan/miniconda3/envs/rl/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PPO' object has no attribute 'get_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m SingleAgentRobotWarehouse()\n\u001b[1;32m      3\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(env, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5e-4\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, ent_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rewards)\n",
      "Cell \u001b[0;32mIn[3], line 98\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(states) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m     97\u001b[0m         returns, advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gae(rewards, values, dones)\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m         states, actions, log_probs, rewards, dones, values \u001b[38;5;241m=\u001b[39m [], [], [], [], [], []\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episode_rewards\n",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self, states, actions, log_probs, returns, advantages)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, actions, log_probs, returns, advantages):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_epochs):\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batches\u001b[49m(states, actions, log_probs, returns, advantages):\n\u001b[1;32m     43\u001b[0m             state_batch, action_batch, old_log_probs_batch, return_batch, advantage_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;66;03m# Calculate the new log probabilities and values\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPO' object has no attribute 'get_batches'"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "env = SingleAgentRobotWarehouse()\n",
    "ppo = PPO(env, lr=2.5e-4, gamma=0.99, epsilon=0.1, ent_coef=0.01)\n",
    "rewards = ppo.train(total_timesteps=1_000_000)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('PPO Training Progress')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
